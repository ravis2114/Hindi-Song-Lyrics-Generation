{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lyrics_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJgA7XzAOK54"
      },
      "source": [
        "# creating TPU environment to create model architecture and initialize architecture's variable on TPU\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# create a distribution stratagy\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWQvDnzgOUk6"
      },
      "source": [
        "#importing basic libraries\n",
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#generated data path\n",
        "txt_gen = '/content/drive/MyDrive/Colab Notebooks/Data_Science_Projects/txt_gen/'\n",
        "\n",
        "# reading dataset generated using clean_data.py file (train_songs.txt)\n",
        "with open(txt_gen + 'train_songs.txt', encoding='utf-8') as f:\n",
        "  df = f.read()\n",
        "  df = df.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdPntfFDEtIc",
        "outputId": "1228af89-b06c-4328-efee-c3b83d41eef5"
      },
      "source": [
        "type(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLlc2fkAFKnV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLbpuLP8OgMD"
      },
      "source": [
        "# tokenizing list of sentences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df)\n",
        "sequences = tokenizer.texts_to_sequences(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4zHd0hBFWk9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDvPRzixOoxP"
      },
      "source": [
        "# final training dataset\n",
        "seq = np.array(sequences)\n",
        "x,y = seq[:,:-1], seq[:,-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LigQEPa9Opvp"
      },
      "source": [
        "# vocab size is total number of unique words plus one for unknown word if present(this is important for embedding layer)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsV1q2LzOsG7"
      },
      "source": [
        "#model Architecture\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import  Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "\n",
        "#model creation to use in TPU\n",
        "def create_model():\n",
        "  return tf.keras.Sequential(\n",
        "      [Embedding(vocab_size, 69, input_length=x.shape[1]), #69 is embedding dimension\n",
        "       LSTM(128, return_sequences=True),\n",
        "       LSTM(128),\n",
        "       Dense(100, activation='relu'),\n",
        "       Dense(vocab_size, activation='softmax')])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crWb-i7rOyhU"
      },
      "source": [
        "#Note that Keras model creation needs to be inside strategy.scope, so the variables can be created on each TPU device. Other parts of the code is not necessary to be inside the strategy scope.\n",
        "\n",
        "# creating model inside TPU\n",
        "with strategy.scope():\n",
        "  # model = create_model()\n",
        "  # loss_fn=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  # opt = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=1)\n",
        "  # model.compile(optimizer=opt, loss=loss_fn, metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "  #load model if you want to train pre-trained model\n",
        "  model = load_model('/content/drive/MyDrive/Colab Notebooks/Data_Science_Projects/lyrics.h5') #comment this and use above method to create model if you want to create a fresh model to train\n",
        "  # loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  # opt = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.59, beta_2=0.8999, clipnorm=0.85)\n",
        "  # model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9clUiuIYO7Jj",
        "outputId": "27cf63d5-aee5-42e2-c3cd-ae92df123236"
      },
      "source": [
        "#training...\n",
        "model.fit(x=x, y=y, batch_size=1024, epochs=2)\n",
        "#save model\n",
        "model.save('/content/drive/MyDrive/Colab Notebooks/Data_Science_Projects/lyrics.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "663/663 [==============================] - 14s 21ms/step - loss: 1.7297 - sparse_categorical_accuracy: 0.6391\n",
            "Epoch 2/2\n",
            "663/663 [==============================] - 14s 21ms/step - loss: 1.7332 - sparse_categorical_accuracy: 0.6388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8PP96rKO8A8"
      },
      "source": [
        "# generating song lyrics (next 50 words..., change this value accordingly)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxTqa1xhoTm3"
      },
      "source": [
        "###### our input part ###########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXsHu8GKR3O1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us6v63xJPBvL"
      },
      "source": [
        "# randon seed text from google search\n",
        "sng = 'तू आता है सीने में जब जब सांसें भारती हूँ' #'मेरी नज़र का सफ़र तुझपे ही आके रुके कहने को तुझपे ही जाके' #'तू आता है सीने में जब जब सांसें भारती हूँ' use this as another example song from the movie MS DHONI\n",
        "seed_text = tokenizer.texts_to_sequences([sng])[0] #sequences[randint(0,len(sequences))] use from training data itself if not from google search or typing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y-LfUF-ofnk"
      },
      "source": [
        "###### now our model will generate lyrics ##################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3FLMpKePEb2"
      },
      "source": [
        "generated_song = []\n",
        "for i in range(50):\n",
        "  input_text = seed_text\n",
        "  pad_seq = pad_sequences([input_text],maxlen=10, truncating='pre') #after appending after next iteration, it removes(tranucates) all words left to last 10(maxlen, change this value based on your sequencial data) words\n",
        "  pred = model.predict(pad_seq) # outputs vector of length of vocab_size\n",
        "  input_text.append(np.argmax(pred)) #argmax gets the index of maximum value\n",
        "  generated_song.append(tokenizer.index_word[np.argmax(pred)]) # value of index of max value is supplied to index to word dict generated after fitting tokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz7g8zYsPHW_",
        "outputId": "79e5879e-387b-4818-c24f-2c4d611b6092"
      },
      "source": [
        "print('-------seed text---------------')\n",
        "print(tokenizer.sequences_to_texts([seed_text[:10]])) #seed text, slicing is being done here coz during appending input_text, seed_text also got appended\n",
        "\n",
        "print('-------generated text---------------')\n",
        "for i in range(0,len(generated_song),10):\n",
        "  print(' '.join(generated_song[i:i+10]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------seed text---------------\n",
            "['तू आता है सीने में जब जब सांसें भारती हूँ']\n",
            "-------generated text---------------\n",
            "तेरे दिल की गलियों से मैं हर रोज़ गुज़रती हूँ\n",
            "पर कहतें है वो क्या किया इश्क़ का फासलों में\n",
            "भी आये है वक़्त तो दीजिये जहाँ तुझको बस में\n",
            "उतारा है कि नहीं मेरा रास्ता हो हो हो हो\n",
            "चाहत का झगड़ा हे हो ओ ओ हो हो ठुकराओ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}